{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Privatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The privacy of users is one of the most important aspects of maintaining customer trust. Recently, researchers have demonstrated that it is [sometimes](https://arxiv.org/abs/1610.05820) [possible](https://arxiv.org/abs/1811.00513) to extract user data from machine learning models. In this solution we will demonstrate that it's possible to train NLP models that help protect the privacy of users, while maintaining high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first notebook we will create a privatized dataset from Amazon product reviews, using Amazon SageMaker Processing. This process involves gathering some statistics from the original dataset, then applying a privatization mechanism that helps protect the privacy of users when the privatized data are used to train downstream models.\n",
    "\n",
    "We start by explaining how the privatization algorithm we will use works. The algorithm was developed by [Amazon scientists](https://www.amazon.science/blog/preserving-privacy-in-analyses-of-textual-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of the privatization mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of the privatization algorithm we'll be using, is to replace the sensitive text in a sentence with other words that are semantically similar. We do this by moving in embedding space, from the original word, towards a carefully crafted noise vector, and obtaining a new word. In the example below, we replace the word \"phone\" with the word \"mobile\". The technique is a form of _differential privacy_ that perturbs data in way such that attackers cannot claim with certainty whether a particular sentence originated from a user, or was the result of a perturbation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example of noise injection](./images/privatization-example.png)\n",
    "Image Source: https://www.amazon.science/blog/preserving-privacy-in-analyses-of-textual-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of the algorithm are the following:\n",
    "\n",
    "* For each word in the dataset:\n",
    "    * Retrieve the word's embedding vector $w$. In this example we use [GloVe](https://nlp.stanford.edu/projects/glove/) 300-dimensional embeddings.\n",
    "    * Generate a noisy vector $\\delta$, using Laplacian noise. The parameter `epsilon` determines the amount of noise added.\n",
    "        * We use this noise to find embeddings of similar words that are close to the original we're trying to replace.\n",
    "    * Retrieve the embedding vector that is closest to the noisy vector $w + \\delta$.\n",
    "    * Get the word that corresponds to that closest vector.\n",
    "        * For example, the closest word to Germany + `noise_vector` might end being France.\n",
    "    * Replace the original word with the retrieved word that was closest to the noisy embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The mechanism we just described requires the following artifacts to work:\n",
    "\n",
    "* A word-index mapping. This allows us to map word strings to vectors and back. Here we use a `torchtext.Vocab` object.\n",
    "* An approximate nearest neighbor index. This allows us to quickly find the words that are close to the noisy vector in our embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To quickly illustrate the algorithm at work, we include pre-trained files in the solution. We will use these files \n",
    "to create interactive privatization examples, then perform the pre-processing ourselves on the Amazon Review data, using Amazon SageMaker Processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the notebook's dependencies\n",
    "import sys\n",
    "sys.path.append('./src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from package import config\n",
    "solutions_bucket = f\"{config.SOLUTIONS_S3_BUCKET}-{config.AWS_REGION}\"\n",
    "solution_name = config.SOLUTION_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the required artifacts\n",
    "!aws s3 sync s3://$solutions_bucket/$solution_name/artifacts/ ./artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This function will replace a single word with a privatized version\n",
    "from package.data_privatization.data_privatization import replace_word\n",
    "\n",
    "# The torchtext vocab contains mappings from vectors to words and back\n",
    "from torchtext import vocab\n",
    "import torch\n",
    "from os.path import join\n",
    "\n",
    "artifact_prefix = \"./artifacts\"\n",
    "train_vocab = torch.load(join(artifact_prefix, \"vocab.pt\"))\n",
    "embedding_dims = 300 # Because we use the 300-dim GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We use the [annoy](https://github.com/spotify/annoy/) library to find the nearest vectors quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "ann_index = AnnoyIndex(embedding_dims, 'euclidean')\n",
    "assert ann_index.load(join(artifact_prefix, \"index.ann\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we have everything in place to see the privatization mechanism in action. The `count_replacements` function below will take a word as input with a couple of parameters and use the mechanism to return a privatized version of the word.\n",
    "\n",
    "The `epsilon` value determines the amount of noise introduced. Smaller values of `epsilon` means more noise is added,\n",
    "making it less likely we'll get the original word back as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to count the replacements for one word\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def count_replacements(word, epsilon, num_replacements=100):\n",
    "    if train_vocab.stoi[word] == 0:\n",
    "        print(\"WARNING: You chose an out-of-vocabulary word, the returned words will be unrelated.\")\n",
    "    replacement_counter = Counter()\n",
    "    for i in range(num_replacements):\n",
    "        replacement_counter[replace_word(word, train_vocab, epsilon, ann_index, embedding_dims)] += 1\n",
    "    percentages = [count/num_replacements for count in sorted(replacement_counter.values(), reverse=True)]\n",
    "    counts = pd.DataFrame(replacement_counter.most_common(), columns=[\"Word\", \"Count\"])\n",
    "    counts['Ratio'] = percentages\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Privatization example.\n",
    "\n",
    "\n",
    "Let us use an example to demonstrate the process of privatization for a single word.\n",
    "You can choose a word to replace, and play around with the `epsilon` value, which determines the amount\n",
    "of noise introduced to the original embedding vector.\n",
    "You will observe that as you change the `epsilon` value to smaller values you will get a larger variety of words back,\n",
    "while changing it to larger values will generally tend to return the original word. Feel free to replace the original word with your own, but note that the vocabulary only has 25,000 words that are present in the Amazon review data, so you'll have more luck using common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "count_replacements(word=\"germany\", epsilon=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will perform the steps that are necessary to produce the privatization artifacts we just used above, and privatize a dataset of\n",
    "Amazon reviews. In the next notebook, we will use these reviews to create two sentiment classification models, one\n",
    "trained on the privatized data and one on the original data, and compare their performance in terms of utility and\n",
    "privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform privatization process on review data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our next step is to perform the privatization process demonstrated above to every review in the Amazon reviews dataset.\n",
    "Let's first set up our environment with the input and output buckets on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "from package import config, container_build\n",
    "\n",
    "# We create a SageMaker session and get the IAM role we'll be using\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = config.PRIVACY_SAGEMAKER_IAM_ROLE\n",
    "\n",
    "# Get the input and output buckets\n",
    "output_bucket = config.S3_BUCKET\n",
    "solution_prefix = config.SOLUTION_NAME\n",
    "prefix = solution_prefix\n",
    "\n",
    "# These are the embeddings that we'll use for our privatization mechanism.\n",
    "s3_vectors = \"s3://{}/{}/vectors/glove.6B.300d.txt.gz\".format(solutions_bucket, solution_prefix)\n",
    "# The input training data lies on S3\n",
    "sensitive_train_data = \"s3://{}/{}/data/train_examples.csv\".format(solutions_bucket, solution_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up our output destinations for the outcomes of the privatization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Here is where our processed data will go\n",
    "processed_data = 's3://{}/{}/processed-data'.format(output_bucket, prefix)\n",
    "# And here will go the artifacts created by the privatization mechanism, we can use those to privatize new inputs\n",
    "artifacts = 's3://{}/{}/artifacts'.format(output_bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download a sample of the data and take a look at one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "data_sample = \"s3://{}/{}/data/train_1k.csv\".format(solutions_bucket, solution_prefix)\n",
    "S3Downloader.download(s3_uri=data_sample, local_path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -2 train_1k.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an example review above, with the sentiment at the end, 1 for negative, 2 for positive.\n",
    "Let's move on to performing data the privatization now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build container for data privatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For our next step we'll prepare the Docker container that we will use to run the privatization on our data.\n",
    "To ensure that our approach is scalable we are using Apache Spark to parallelize the privatization process.\n",
    "You can view the complete script under `src/package/data_privatization/data_privatization.py` that applies the steps we did above for every sentence in the Amazon reviews\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset we use is only 25,000 records, so we run Spark locally on a single node, making use of all its cores.\n",
    "If your dataset is very large (billions of records) you might want to use distributed processing to process the complete dataset quickly. For more information on using a Spark container with Amazon SageMaker see [here](https://docs.aws.amazon.com/sagemaker/latest/dg/use-spark-processing-container.html). The process of building the container should take around **5 minutes** to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "region = config.AWS_REGION\n",
    "account_id = config.AWS_ACCOUNT_ID\n",
    "\n",
    "ecr_repository = config.SAGEMAKER_PROCESSING_JOB_CONTAINER_NAME\n",
    "\n",
    "if config.SAGEMAKER_PROCESSING_JOB_CONTAINER_BUILD == \"local\":\n",
    "    old_cwd = os.getcwd()\n",
    "    os.chdir(\"./src/package/data_privatization/\")\n",
    "    !bash container/build_and_push.sh $ecr_repository $region $account_id\n",
    "    os.chdir(old_cwd)\n",
    "else:\n",
    "    container_build.build(config.SAGEMAKER_PROCESSING_JOB_CONTAINER_BUILD)\n",
    "\n",
    "ecr_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account_id, region, ecr_repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run data privatization job with Amazon SageMaker Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that our container is built, we can run the privatization job. The following cell will launch an instance using the\n",
    "container we just created, and execute the `data_privatization.py` script. \n",
    "\n",
    "This will create a new privatized dataset based\n",
    "on the original data, as well as a set of output artifacts we can use to privatize words and sentences on the fly, replicating the results of the demo above.\n",
    "It will also lightly pre-process the original data and create new output that we will use to train a model on in the next notebook.\n",
    "\n",
    "Note: The cell below should take around **15 minutes** to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                                   sagemaker_session=sagemaker_session,\n",
    "                                   image_uri=ecr_repository_uri,\n",
    "                                   role=role,\n",
    "                                   instance_count=1,\n",
    "                                   instance_type='ml.c5.4xlarge')\n",
    "\n",
    "script_processor.run(code='src/package/data_privatization/data_privatization.py',\n",
    "                     inputs=[ProcessingInput(source=sensitive_train_data,\n",
    "                                             destination='/opt/ml/processing/input'),\n",
    "                            ProcessingInput(source=s3_vectors,\n",
    "                                             destination='/opt/ml/processing/vectors')],\n",
    "                     outputs=[ProcessingOutput(destination=processed_data,\n",
    "                                               source='/opt/ml/processing/output'),\n",
    "                             ProcessingOutput(destination=artifacts,\n",
    "                                               source='/opt/ml/processing/artifacts')],\n",
    "                    arguments=['--epsilon', '23'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## View results of privatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the above process has finished we have access to two new CSV files: one with original reviews and one with the privatized version of each review. We will use these two files to train separate models in the next notebook. For now, let's take a look at one example output from each of the created training files on S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sensitive_sample = processed_data + \"/reviews-sensitive/part-00000\"\n",
    "privatized_sample = processed_data + \"/reviews-privatized/part-00000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!aws s3 cp $sensitive_sample sensitive_sample.csv\n",
    "!aws s3 cp $privatized_sample privatized_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!tail -1 sensitive_sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!tail -1 privatized_sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see in the above comparison that many words will remain the same, and many will change. While individual reviews no longer make grammatical sense after the privatization, in aggregate they should maintain the original review's sentiment. We put this to the test in the next notebook where we train two sentiment classification models, one on the original data and one on the privatized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the epsilon parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important decisions one has to make when using differential privacy is setting the `epsilon` value that determines the amount of noise added to the data. This is largely an open research problem, and even harder for privacy in metric spaces, such as embeddings, because the effect of epsilon will depend on the density of the embeddings being used (see the [linked article](https://www.amazon.science/blog/preserving-privacy-in-analyses-of-textual-data) for more information). \n",
    "\n",
    "In this notebook we follow the suggestions of the original publication for the Glove-300 embeddings and set an epsilon value of 23. A good rule of thumb would be to set a business goal of how much utility loss is acceptable in the downstream task you're interested in, for example %2 absolute accuracy loss on the test set, then setting the epsilon value to the lowest possible value that maintains the desired accuracy level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up: Training and comparing original and privatized models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook we will use the two datasets we have created here to train two separate models to predict the sentiment of the reviews.\n",
    "Finally, will then investigate how the privatization mechanism affects the accuracy of the models. You can move directly to [Notebook 2](./2.Model_Training.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch JumpStart) (sagemaker-jumpstart-pytorch/latest)",
   "language": "python",
   "name": "HUB_1P_IMAGE"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}